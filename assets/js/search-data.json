{
  
    
        "post0": {
            "title": "Natural Language Inference with PyTorch and Transformers",
            "content": "In this post I&#39;m showing how to use PyTorch and Huggingface Transformers to fine-tune a pre-trained transformers model to do natural language inference (NLI). In NLI the aim is to model the inferential relationship between two or more given sentences. In particular, given two sentences - the premise p and the hypothesis h - the task is to determine whether h is entailed by p, whether the sentences are in contradiction with each other or whether there is no inferential relationship between the sentences (neutral). . So let&#39;s get started! First we need to install the python libraries using the following command. . !pip3 install torch transformers datasets . We will then import the needed libraries. We are using DistilBERT model for this task so we need to import the relevant DistilBERT model designed for sequence classification task and the corresponding tokeniser. . import torch from torch.utils.data import DataLoader from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW import datasets from tqdm import tqdm import numpy as np . Lets load the MultiNLI dataset using the Huggingface Datasets library. For this demonstration we are using only the treaining and validation data. We are also further limiting the training data to just 20,000 sentence pairs. This will not allow us to train a good quality model, but it speeds up the demonstration. You can change the values here or use the whole dataset. However, be aware that fine tuning the model will take a lot of time. . nli_data = datasets.load_dataset(&quot;multi_nli&quot;) train_data = nli_data[&#39;train&#39;][:20000] # limiting the training set size to 20,000 for demo purposes train_labels = train_data[&#39;label&#39;][:20000] # limiting the training set size to 20,000 for demo purposes dev_data = nli_data[&#39;validation_matched&#39;] val_labels = dev_data[&#39;label&#39;] . Next we will initialise the tokeniser and tokenise our training and validation data. Notice that we are two lists of sentences to both the training and validation set. This is because in NLI we are classifying pairs of sentences: the premise and the hypothesis. . tokeniser = DistilBertTokenizerFast.from_pretrained(&#39;distilbert-base-uncased&#39;) train_encodings = tokeniser(train_data[&#39;premise&#39;], train_data[&#39;hypothesis&#39;], truncation=True, padding=True) val_encodings = tokeniser(dev_data[&#39;premise&#39;], dev_data[&#39;hypothesis&#39;], truncation=True, padding=True) . Once the data has been tokenised we will create a NLIDataset object for our data. Here we are creating a subclass that inherits the torch.utils.data.Dataset class. . class NLIDataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.encodings.input_ids) . Once we&#39;ve defined our dataset class we can initialise the training and validation datasets with our tokenised sentence pairs and labels. We will then create DataLoader objects for the training and validation data. . train_dataset = NLIDataset(train_encodings, train_labels) val_dataset = NLIDataset(val_encodings, val_labels) train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True) . Now, before we can start training, we need to import our model and optimiser to be used in training. We first set the device and use cuda if GPU is available. We then get the pre-trained DistilBERT model specifying the number of classes we are classifying to. . device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;) model = DistilBertForSequenceClassification.from_pretrained(&#39;distilbert-base-uncased&#39;, num_labels=3) model.to(device) model.train() optim = AdamW(model.parameters(), lr=5e-5) . Now we are ready to train the model. In this demonstration we are fine-tuning for just three epochs, but you can change the value to something more meaningful if you like. Note that you could also use the Transformers Trainer class to fine-tune the model but I&#39;ve chosen to use native PyTorch instead. . epochs = 3 for epoch in range(epochs): all_losses = [] for batch in tqdm(train_loader, total=len(train_loader), desc=&quot;Epoch: {}/{}&quot;.format(epoch+1, epochs)): optim.zero_grad() input_ids = batch[&#39;input_ids&#39;].to(device) attention_mask = batch[&#39;attention_mask&#39;].to(device) labels = batch[&#39;labels&#39;].to(device) outputs = model(input_ids, attention_mask=attention_mask, labels=labels) loss = outputs[0] loss.backward() optim.step() all_losses.append(loss.item()) print(&quot; nMean loss: {:&lt;.4f}&quot;.format(np.mean(all_losses))) . Once the model has been trained we can evaluate it to get the validation accuracy for our model. . model.eval() with torch.no_grad(): eval_preds = [] eval_labels = [] for batch in tqdm(val_loader, total=len(val_loader)): input_ids = batch[&#39;input_ids&#39;].to(device) attention_mask = batch[&#39;attention_mask&#39;].to(device) labels = batch[&#39;labels&#39;].to(device) preds = model(input_ids, attention_mask=attention_mask, labels=labels) preds = preds[1].argmax(dim=-1) eval_preds.append(preds.cpu().numpy()) eval_labels.append(batch[&#39;labels&#39;].cpu().numpy()) print(&quot; nValidation accuracy: {:6.2f}&quot;.format(round(100 * (np.concatenate(eval_labels) == np.concatenate(eval_preds)).mean()), 2)) . Now we are all done. As you can see the results are far from state of the art if you use just a fraction of the training data. . Hope you enjoyed this demo. Feel free to contact me if you have any questions. . Twitter: @AarneTalman | Website: talman.io | .",
            "url": "https://talman.io/2020/12/11/natural-language-inference-with-pytorch-and-transformers.html",
            "relUrl": "/2020/12/11/natural-language-inference-with-pytorch-and-transformers.html",
            "date": " • Dec 11, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I’m a researcher in Language Technology at University of Helsinki. My research focuses on computational semantics, natural language understanding, machine translation and machine learning. . I work as the CTO at Nordcloud UK. I have 15 years of experience in software development, architecture, management consulting, startup business, AI/ML and NLP development and academic research. . Education . 2018 - present, PhD in Language Technology, University of Helsinki | 2005 - 2007, MSc in Computational Linguistics and Formal Grammar, King’s College London Graduated with Distinction. | 2002 - 2005, BSc in Philosophy, London School of Economics Graduated with First Class Honours. | . Employment . 2020 - present, CTO, Nordcloud UK | 2018 - 2020, Doctoral Researcher, Language Technology, University of Helsinki Working on computational semantics and natural language processing. | 2019 - present, Founder, Basement AI Basement AI is a Nordic artificial intelligence research lab specializing in natural language processing and machine learning. | 2015 - 2018, Associate Director, Consulting, Gartner. | 2012 - 2015, Consultant, Accenture. | 2011 - 2012, Research Student, London School of Economics. | 2009 - 2011, Product Manager, Nokia. | 2008 - 2009, Manager, Nokia. | 2006 - 2008, Systems Analyst, Tieto. | 2006 - 2006, Software Developer, Valuatum. | .",
          "url": "https://talman.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications and Talks",
          "content": "Academic Publications . Aarne Talman, Antti Suni, Hande Celikkanat, Sofoklis Kakouros, Jörg Tiedemann and Martti Vainio. 2019. Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations. Proceedings of NoDaLiDa 2019. [bibtex] [pdf] [corpus and code] | Aarne Talman, Umut Sulubacak, Raúl Vázquez, Yves Scherrer, Sami Virpioja, Alessandro Raganato, Arvi Hurskainen, and Jörg Tiedemann. 2019. The University of Helsinki submissions to the WMT19 news translation task. Proceedings of the Fourth Conference on Machine Translation: Shared Task Papers. [bibtex] [pdf] | Aarne Talman and Stergios Chatzikyriakidis. 2019. Testing the Generalization Power of Neural Network Models Across NLI Benchmarks. Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. [bibtex] [pdf] | Aarne Talman, Anssi Yli-Jyrä and Jörg Tiedemann. 2019. Sentence Embeddings in NLI with Iterative Refinement Encoders. Natural Language Engineering 25(4). [bibtex] [pdf] [code] | Academic Talks . Predicting Prosodic Prominence from Text with Pre-trained Contextualized Word Representations. 2 October 2019, NodaLiDa 2019, Turku. [pdf] | Neural Network models of NLI fail to capture the general notion of inference, 8 March 2019, CLASP Seminar, University of Gothenburg. [pdf] | State-of-the-Art Natural Language Inference Systems Fail to Capture the Semantics of Inference, 25 October 2018, Research Seminar in Language Technology, University of Helsinki. [pdf] | Natural Language Inference with Hierarchical BiLSTM’s, 28 September 2018, FoTran 2018. [pdf] | Natural Language Inference - Another Triumph for Deep Learning?, 23 November 2017, Research Seminar in Language Technology, University of Helsinki. [pdf] |",
          "url": "https://talman.io/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Resources",
          "content": "Software . Prosody: A system for predicting prosodic prominence from written text. License: MIT | Paper | . | Natural Language Inference: Natural language inference system written in Python and PyTorch implementing the HBMP sentence encoder. License: MIT | Paper | . | Data . Helsinki Prosody Corpus: The prosody corpus contains automatically generated, high quality prosodic annotations for the LibriTTS corpus (Zen et al. 2019) using the Continuous Wavelet Transform Annotation method (Suni et al. 2017). Language: English | License: CC BY 4.0 | Paper | . |",
          "url": "https://talman.io/resources/",
          "relUrl": "/resources/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page9": {
          "title": "Teaching",
          "content": "University of Helsinki . Instructor: Approaches to natural language understanding. 2019-2019, Period 4. | Supervision: Learning and evaluation of multilingual rentence representations (Master Thesis) | Reading Group: Representation Learning for Natural Language Understanding. Fall 2019. | Teaching Assistant: KIK-LG210 Machine Learning for Linguists. 2018-2019, Period 4. | Teaching Assistant: LDA-T3115 A Practical Introduction to Modern Neural Machine Translation. 2018-2019, Period 4. | . Other . Lab monitor / TA: 2019 Lisbon Machine Learning School (LxMLS 2019). | .",
          "url": "https://talman.io/teaching/",
          "relUrl": "/teaching/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://talman.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}