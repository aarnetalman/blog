{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSvPmQT5aWUz"
   },
   "source": [
    "# Natural Language Inference with PyTorch and Transformers\n",
    "> Demo on how to use PyTorch and Transformers in the NLI task\n",
    " \n",
    "- toc: false\n",
    "- comments: true\n",
    "- author: Aarne Talman\n",
    "- categories: [nli, pytorch, demo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tTCy6hInCFy"
   },
   "source": [
    "In this notebook I'm showing how to use [PyTorch](https://pytorch.org/) and [Huggingface Transformers](https://github.com/huggingface/transformers) to fine-tune a pre-trained transformers model to do natural language inference (NLI). In NLI the aim is to model the inferential relationship between two or more given sentences. In particular, given two sentences - the premise `p` and the hypothesis `h` - the task is to determine whether `h` is entailed by `p`, whether the sentences are in contradiction with each other or whether there is no inferential relationship between the sentences (neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEigYdvNm6fw"
   },
   "source": [
    "So let's get started! First we need to install the python libraries using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQsnUmUYbWg-"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8aLgMRDcW4G"
   },
   "source": [
    "We will then import the needed libraries. We are using [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5) model for this task so we need to import the relevant DistilBERT model designed for sequence classification task and the corresponding tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jTeqGSOw-hPb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW, logging\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0Z8MelYdyZP"
   },
   "source": [
    "Let's load the [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) dataset using the Huggingface [Datasets](https://github.com/huggingface/datasets) library. For this demonstration we are using only the training and validation data. We are also further limiting the training data to just 20,000 sentence pairs. This will not allow us to train a good quality model, but it speeds up the demonstration. You can change the values here or use the whole dataset. However, be aware that fine tuning the model will take a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL14Lz7RnguI"
   },
   "outputs": [],
   "source": [
    "nli_data = datasets.load_dataset(\"multi_nli\")\n",
    "\n",
    "train_data = nli_data['train'][:20000] # limiting the training set size to 20,000 for demo purposes\n",
    "train_labels = train_data['label']\n",
    "\n",
    "dev_data = nli_data['validation_matched']\n",
    "val_labels = dev_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2KDJEuUc8tx"
   },
   "source": [
    "Next we will initialise the tokeniser and tokenise our training and validation data. Notice that we are two lists of sentences to both the training and validation set. This is because in NLI we are classifying pairs of sentences: the premise and the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YykDENnn8r5"
   },
   "outputs": [],
   "source": [
    "tokeniser = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokeniser(train_data['premise'], train_data['hypothesis'], truncation=True, padding=True)\n",
    "val_encodings = tokeniser(dev_data['premise'], dev_data['hypothesis'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDZgCYAnfMPV"
   },
   "source": [
    "Once the data has been tokenised we will create a `NLIDataset` object for our data. Here we are creating a subclass that inherits the `torch.utils.data.Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5MfiMear4JP3"
   },
   "outputs": [],
   "source": [
    "class NLIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQQttj4agcb5"
   },
   "source": [
    "Once we've defined our dataset class we can initialise the training and validation datasets with our tokenised sentence pairs and labels. We will then create `DataLoader` objects for the training and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0WwUbwEuurQ3"
   },
   "outputs": [],
   "source": [
    "train_dataset = NLIDataset(train_encodings, train_labels)\n",
    "val_dataset = NLIDataset(val_encodings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Qf5L3jjgzev"
   },
   "source": [
    "Now, before we can start training, we need to import our model and optimiser to be used in training. We first set the device and use `cuda` if GPU is available. We then get the pre-trained DistilBERT model specifying the number of classes we are classifying to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_a9XDjCvIcM"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnoVKIXyheTn"
   },
   "source": [
    "Now we are ready to train the model. In this demonstration we are fine-tuning for just three epochs, but you can change the value to something more meaningful if you like. Note that you could also use the Transformers `Trainer` class to fine-tune the model but I've chosen to use native PyTorch instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YHKhgarBvoRU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3: 100%|██████████| 1250/1250 [15:31<00:00,  1.34it/s]\n",
      "Epoch: 2/3:   0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean loss: 0.8789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3: 100%|██████████| 1250/1250 [15:27<00:00,  1.35it/s]\n",
      "Epoch: 3/3:   0%|          | 0/1250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean loss: 0.5912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3: 100%|██████████| 1250/1250 [15:27<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean loss: 0.3316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    all_losses = []\n",
    "\n",
    "    for batch in tqdm(train_loader, total=len(train_loader), desc=\"Epoch: {}/{}\".format(epoch+1, epochs)):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        all_losses.append(loss.item())\n",
    "        \n",
    "    print(\"\\nMean loss: {:<.4f}\".format(np.mean(all_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjr6KRhahvTp"
   },
   "source": [
    "Once the model has been trained we can evaluate it to get the validation accuracy for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JwFKC9jiu7-Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614/614 [02:26<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy:  69.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_preds = []\n",
    "    eval_labels = []\n",
    "\n",
    "    for batch in tqdm(val_loader, total=len(val_loader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        preds = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        preds = preds[1].argmax(dim=-1)\n",
    "        eval_preds.append(preds.cpu().numpy())\n",
    "        eval_labels.append(batch['labels'].cpu().numpy())\n",
    "\n",
    "print(\"\\nValidation accuracy: {:6.2f}\".format(round(100 * (np.concatenate(eval_labels) == np.concatenate(eval_preds)).mean()), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHdUoWm2h8G9"
   },
   "source": [
    "Now we are all done. As you can see the results are far from state of the art if you use just a fraction of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcHYo14Lvgjo"
   },
   "source": [
    "Hope you enjoyed this demo. Feel free to contact me if you have any questions.  \n",
    "*   Twitter: [@AarneTalman](https://twitter.com/aarnetalman)\n",
    "*   Website: [talman.fi](https://talman.fi)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-12-11-natural-language-inference-with-pytorch-and-transformers.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
