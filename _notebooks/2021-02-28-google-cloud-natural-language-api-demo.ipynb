{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hispanic-madrid",
   "metadata": {},
   "source": [
    "# Analysing News Article Content with Google Cloud Natural Language API\n",
    "> Demo on how to use GCP Natural Language and AI Platform Notebooks\n",
    " \n",
    "- toc: false\n",
    "- comments: true\n",
    "- author: Aarne Talman\n",
    "- categories: [GCP, Cloud Natural Language, demo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-arnold",
   "metadata": {},
   "source": [
    "In my [previous blog post](https://talman.io/2021/02/25/training-transformer-model-gcp-ai-platform.html) I showed how to use AI Platform Training to fine-tune a custom NLP model using PyTorch and the `transformers` library. In this post we take advantage of Google's pre-trained AI models for NLP and use [Cloud Natural Language API](https://cloud.google.com/natural-language/docs) to analyse text. Google's pre-trained machine learning APIs are great for building working AI prototypes and proof of concepts in matter of hours.\n",
    "\n",
    "Google's Cloud Natural Language API allows you to do named entity extractions, sentiment analysis, content classification and syntax analysis using a simple REST API. The API supports Python, Go, Java, Node.js, Ruby, PHP and C#. In this post we'll be using the Python API.\n",
    "\n",
    "Before we jump in, let's define our use case. To highlight the simplicity and power of the API I'm going to use it to analyse the content of news articles. In particular I want to find out if the latest articles published in The Guardian's world news section contain mentions of \"famous people\" and if those mentions are have a positive or a negative sentiment. I also want to find out the overall sentiment of the news articles. To do this, we will go through a number of steps. We will\n",
    "1. Use the Guardian's RSS feed to extract links to the latest news articles in the world news section.\n",
    "2. Download the HTML content of the articles published in the past 24 hours and extract the article text in plain text.\n",
    "3. Analyse the overall sentiment of the text using Cloud Natural Language.\n",
    "4. Extract named entities from the text using Cloud Natural Language.\n",
    "5. Go through all named entities of type `PERSON` and see if they have a Wikipedia entry (for the purposes of this post, this will be our measure of the person being \"famous\").\n",
    "6. Once we've identified all the mentions of \"famous people\", we analyse the sentiment of the sentences mentioning them.\n",
    "7. Finally, we will print the names, Wikipedia links and the sentiments of the mentions of all the \"famous people\" in each article, together with the article title, url and the overall sentiment of the article.\n",
    "\n",
    "We will do all this using GCP AI Platform Notebooks. To launch new notebook make sure you have an active project selected. Navigate to AI Platform Notebooks and select **New Instance**. For this demo you don't need a very powerful notebook instance, so we will make some changes to the defaults to save cost. First, select **Python 3** (without CUDA) from the list and give a name for your notebook. Next, click the edit icon next to **Instance properties**. From Instance properties select N1-standard-1 as the intance type. You'll see that the estimated cost of running this instance is only \\$0.041.\n",
    "![Instance type](https://talman.io/images/instance-type.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-scottish",
   "metadata": {},
   "source": [
    "## Steps 1-2: Extract the Latest News Articles\n",
    "\n",
    "We start start by downloading some required Python libraries. The following line uses pip to install lxml, Beautiful Soup and feedparser using pip. We use lxml and Beautiful Soup for processing and parsing HTML the content. Feedparser will be used to parse the RSS feed to identify the latest news articles and to get the links to the full tect of those articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml bs4 feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-diameter",
   "metadata": {},
   "source": [
    "Once we have installed the required libraries we need to import them together with the other libraries we need for extracting the news article content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import requests\n",
    "import re\n",
    "import feedparser\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-silicon",
   "metadata": {},
   "source": [
    "Next, we will define the url to the RSS feed as well as the time period we want to limit our search to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = \"https://www.theguardian.com/world/rss\"\n",
    "days = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-richardson",
   "metadata": {},
   "source": [
    "We will then define two functons we will use to extract the main article text from the HTML document. The `text_from_html` function will parse the HTML file, extract the text from that file and use the `tag_visible` function to filter out all but the main article text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['p']:\n",
    "        return True\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def text_from_html(html):\n",
    "    soup = BeautifulSoup(html.content, 'lxml')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-matrix",
   "metadata": {},
   "source": [
    "Once we have defined these functions we will parse the RSS feed, identify the articles published in the past 24 hours and extract the required attributes for those articles. We will need the article title, link, publishing time and, using the functions defined above, the plain text version of the article text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsfeed = feedparser.parse(feed)\n",
    "articles = []\n",
    "\n",
    "# Get all the entries from within the last day\n",
    "entries = [entry for entry in newsfeed.entries if time.time() - time.mktime(entry.published_parsed) < (86400*days)]\n",
    "for entry in tqdm(entries, total=len(entries)):\n",
    "    html = requests.get(entry.link)             \n",
    "    src_text = text_from_html(html)             \n",
    "    article = dict()\n",
    "    article[\"title\"] = entry.title\n",
    "    article[\"link\"] = entry.link\n",
    "    article[\"src_text\"] = src_text\n",
    "    article[\"published\"] = entry.published_parsed\n",
    "    articles.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-feeling",
   "metadata": {},
   "source": [
    "## Steps 3-7: Analyse the Content Using Cloud Natural Language API\n",
    "\n",
    "To use the Natural Language API we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-relevance",
   "metadata": {},
   "source": [
    "Next, we define the main function for the demo. Below, in 21 lines of code, we will do all the needed text analysis as well as print the results to view the output. The function takes `document` as the input, analyses the contents and prints the results. We will look at the contents of the `document` input later.\n",
    "\n",
    "To use the API we need to initialise the `LanguegeServiceClient`. We then define the encoding type which we need to pass together with the document to the API.\n",
    "\n",
    "The first API call `analyze_entities(document, encoding_type=encoding_type)` takes the input document and the encoding type and returns a response of the following form:\n",
    "```json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      object(Entity)\n",
    "    }\n",
    "  ],\n",
    "  \"language\": string\n",
    "}\n",
    "```\n",
    "\n",
    "We will then call the API to analyse the sentiment of the document as well as to get the sentimens of each sentence in the document. The response has the following form:\n",
    "```json\n",
    "{\n",
    "  \"documentSentiment\": {\n",
    "    object(Sentiment)\n",
    "  },\n",
    "  \"language\": string,\n",
    "  \"sentences\": [\n",
    "    {\n",
    "      object(Sentence)\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The overall document sentiment is stored in `annotations.document_sentiment.score`. We assign the document an overall sentiment POSITIVE if the score is above 0, NEGATIVE if it is less than 0 and NEUTRAL if it is 0. Once we have determined the sentiment we print it. \n",
    "\n",
    "We then go through all the entities identified by the API and create a list of those entities that have the type PERSON. Once we have this list we loop through it and check which ones from that list have `wikipedia_url` in their `metadata_name`. As said, we use this as our measure of that person being \"famous\". When we identify a \"famoud person\" we print the person's name and the link to the Wikipedia entry.\n",
    "\n",
    "We then check the sentiment annotated sentences for occurence of the identified \"famous people\" and use the same values as above to determine the sentiment category of those sentences. Finally we print all the sentiments of all the sentences mentioning the person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentiments(document):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "    \n",
    "    # Get entities from the document\n",
    "    response = client.analyze_entities(document, encoding_type=encoding_type)\n",
    "    # Get sentiment annontations from the document\n",
    "    annotations = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
    "    # Get overall document sentiment score\n",
    "    overall_sentiment = 'POSITIVE' if annotations.document_sentiment.score > 0 else 'NEGATIVE' \\\n",
    "                    if annotations.document_sentiment.score < 0 else 'NEUTRAL'\n",
    "    \n",
    "    print(f\"Overall sentiment: {overall_sentiment}\")\n",
    "    \n",
    "    # Construct a list of entities where the entity type is a PERSON\n",
    "    entities = [entity for entity in response.entities if enums.Entity.Type(entity.type).name == 'PERSON']\n",
    "    \n",
    "    # Loop through persons\n",
    "    for entity in entities:\n",
    "        # Check if the entity has a metadata entry containing a wikipedia link\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            if metadata_name == 'wikipedia_url':\n",
    "                name = entity.name\n",
    "                wiki_url = metadata_value\n",
    "                print(f\"\\nPerson: {name}\")\n",
    "                print(f\"- Wikipedia: {wiki_url}\")\n",
    "                \n",
    "                # Get all sentences mentioning the person\n",
    "                sentences = [sentence for sentence in annotations.sentences if name in sentence.text.content]\n",
    "                \n",
    "                # Display whether the sentences mentioning the person are negative, positive or neutral\n",
    "                for index, sentence in enumerate(sentences):\n",
    "                    sentence_sentiment = 'POSITIVE' if sentence.sentiment.score > 0 else 'NEGATIVE' \\\n",
    "                    if sentence.sentiment.score < 0 else 'NEUTRAL'\n",
    "                    \n",
    "                    print(f\"- Sentence: {index + 1} mentioning {name} is: {sentence_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-minutes",
   "metadata": {},
   "source": [
    "Now that we have extracted the text from the news site and defined the fucntion to analyse the contents of each article, all we need to do is go through the articles and call the function. The input for the fucntion is a dictionary containing the plain text contents of the article, the type of the document (which in our case if PLAIN_TEXT) and the language of the document (which for us is English). We also print the name of each article and the link to the article. \n",
    "\n",
    "For demo purposes we limit the our analysis to the first 3 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"en\"\n",
    "type_ = enums.Document.Type.PLAIN_TEXT\n",
    "\n",
    "# Analyse the latest 5 articles \n",
    "for article in articles[:3]:\n",
    "    print('\\n' + '#'*50 + '\\n')\n",
    "    print(article[\"title\"])\n",
    "    print(article[\"link\"])\n",
    "    document = {\"content\": article[\"src_text\"], \"type\": type_, \"language\": language}\n",
    "    print_sentiments(document)  \n",
    "print('\\n' + '#'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-channels",
   "metadata": {},
   "source": [
    "As you can see the 3 articles we analysed all have an overall negative sentiment. We also found quite a few mentions of people with Wikipedia entries as well as the sentiment of those sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-inspector",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we saw, the Cloud Natural Language API is super simple and powerful tool taht allows us to analyse text with just  a few lines of code. This is great when you are working on a new use case and you need to quickly test the feasibility of an AI-based solution. It is also the go-to resource when you don't have data to train your own machine learning model for the task. However, if you need to create a more custom model for your use case, I recommend using [AutoML Natural Language](https://cloud.google.com/natural-language/automl/docs) or training your own model using AI Platform Training.\n",
    "\n",
    "Hope you enjoyed this demo. Feel free to contact me if you have any questions.  \n",
    "*   Twitter: [@AarneTalman](https://twitter.com/aarnetalman)\n",
    "*   Website: [talman.io](https://talman.io)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
