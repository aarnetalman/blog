<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Simple Bag-of-Words Search Engine in Python | AarneTalman</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Simple Bag-of-Words Search Engine in Python" />
<meta name="author" content="Aarne Talman" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Demo on how to build a search engine using Python and the BM25 algorithm" />
<meta property="og:description" content="Demo on how to build a search engine using Python and the BM25 algorithm" />
<link rel="canonical" href="https://talman.io/information_retrieval/python/demo/2020/12/30/simple-search-engine-with-bm25.html" />
<meta property="og:url" content="https://talman.io/information_retrieval/python/demo/2020/12/30/simple-search-engine-with-bm25.html" />
<meta property="og:site_name" content="AarneTalman" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-30T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://talman.io/information_retrieval/python/demo/2020/12/30/simple-search-engine-with-bm25.html","@type":"BlogPosting","headline":"Simple Bag-of-Words Search Engine in Python","dateModified":"2020-12-30T00:00:00-06:00","datePublished":"2020-12-30T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://talman.io/information_retrieval/python/demo/2020/12/30/simple-search-engine-with-bm25.html"},"author":{"@type":"Person","name":"Aarne Talman"},"description":"Demo on how to build a search engine using Python and the BM25 algorithm","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://talman.io/feed.xml" title="Aarne&lt;span&gt;Talman&lt;/span&gt;" /><!-- the google_analytics_id gets auto inserted from the config file -->


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KNGC46YL32"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KNGC46YL32');
</script>
<!--<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','G-KNGC46YL32','auto');ga('require','displayfeatures');ga('send','pageview');</script>-->

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Aarne&lt;span&gt;Talman&lt;/span&gt;</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/code/">Code</a><a class="page-link" href="/publications/">Publications and Talks</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a><a class="page-link" href="/teaching/">Teaching</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Simple Bag-of-Words Search Engine in Python</h1><p class="page-description">Demo on how to build a search engine using Python and the BM25 algorithm</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-30T00:00:00-06:00" itemprop="datePublished">
        Dec 30, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Aarne Talman</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
    <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
    
      <a class="category-tags-link" href="/categories/#information_retrieval">information_retrieval</a>
      &nbsp;
    
      <a class="category-tags-link" href="/categories/#python">python</a>
      &nbsp;
    
      <a class="category-tags-link" href="/categories/#demo">demo</a>
      
    
    </p>
  

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/aarnetalman/blog/tree/master/_notebooks/2020-12-30-simple-search-engine-with-bm25.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          <div class="px-2">
    <a href="https://colab.research.google.com/github/aarnetalman/blog/blob/master/_notebooks/2020-12-30-simple-search-engine-with-bm25.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-30-simple-search-engine-with-bm25.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In my <a href="https://talman.io/information_retrieval/demo/2020/12/14/simple-semantic-search-engine-with-transformers.html">previous post</a> I showed how to build a simple semantic search engine using pre-trained transformer models.</p>
<p>In this post I'll show how to build a simple search engine using Python and the BM25 ranking algorithm. <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a> is a bag-of-words ranking function designed for information retrieval. It is an enhanced version of the term frequency–inverse document frequency (<a href="https://en.wikipedia.org/wiki/Tf–idf">tf-idf</a>) method.</p>
<p>The basic idea behind tf-idf is that it looks at the frequency of the term in the document (more the better) and it looks at the inverse document frequency (common words are less important). One of the drawbacks of the standard tf-idf is that long documents with the same term frequency are considered less important.</p>
<p>BM25 makes couple of enhancements to the traditional tf-idf. First, BM25 adds a term frequency saturation to tf-idf, limiting the influence of term frequency on the score. Intuitively this means that more frequent the term is the less impact each occurence of it has on the score. Second, BM25 adds a document length weighting, which makes sure that document length doesn't have such a dramatic negative impact on the relevance score as in tf-idf.</p>
<p>Building a BM25-based search engine with Python is easy. There's a great library <code>rank-bm25</code> which implements the scoring algorithm. To demonstrate the use of this library I'm going to build a simple search engine that retrieves article abstracts from <a href="https://arxiv.org">arXiv</a> and searches for the most relevant articles based on a given search term. So, let's get started.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's first install and import the needed Python libraries.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install rank_bm25 nltk numpy feedparser
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">rank_bm25</span> <span class="kn">import</span> <span class="n">BM25Okapi</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">feedparser</span>
<span class="kn">import</span> <span class="nn">time</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will use NLTK to tokenize the documents and to identify stopwords, so we need to download the relevant data files used by NLTK.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we retrieve the documents from arXiv. For this we use the <a href="https://arxiv.org/help/api/">arXiv API</a>. We download 10,000 abstracts from arXiv's Computation and Language (<a href="https://arxiv.org/list/cs.CL/recent">cs.CL</a>) category. The arXiv API returns an Atom feed which we can parse using the <code>feedparser</code> library. This gives us an easy access to the article titles, abstracts, links to the full texts as well as many other useful attributes. Parsing the 10,000 entries will take a couple of minutes.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_docs</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;http://export.arxiv.org/api/query?search_query=cat:cs.CL&amp;start=0&amp;max_results=</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">num_docs</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">feed</span> <span class="o">=</span> <span class="n">feedparser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we remove any special characters, tokenise the article abstracts and remove any stopwords.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="n">tokenized_docs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">feed</span><span class="o">.</span><span class="n">entries</span><span class="p">:</span>
    <span class="c1"># Article abstracts are stored in entries[i].summary </span>
    <span class="n">doc</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">summary</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;([\w].)([\~\!\@\#\$\%\^\&amp;\*\(\)\-\+\[\]\{\}\/</span><span class="se">\&quot;</span><span class="s2">\&#39;\:\;])([\s\w].)&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\\</span><span class="s2">1 </span><span class="se">\\</span><span class="s2">2 </span><span class="se">\\</span><span class="s2">3&quot;</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span> <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
    <span class="n">tokenized_docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can define the scoring function using the <code>rank-mb25</code> library. This function takes the tokenized article abstracts (docs) and the tokenised search term. It also stores the indices for the scores so that we can later link the scores to the correct article.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_bm25_scores</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
    <span class="n">bm25</span> <span class="o">=</span> <span class="n">BM25Okapi</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">bm25</span><span class="o">.</span><span class="n">get_scores</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">scores</span><span class="p">)]</span>
    <span class="n">scores</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scores</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's define our search term and ther number of results we want to retrieve. In this demo we will search for articles about Natural Language Inference and we want to retrieve the top 5 articles.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_results</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">search_term</span> <span class="o">=</span> <span class="s2">&quot;Natural Language Inference&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we tokenise the search term and call the ranking algorithm we defined earlier.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="n">search_term</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">get_bm25_scores</span><span class="p">(</span><span class="n">tokenized_docs</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">query_time</span> <span class="o">=</span> <span class="n">t1</span><span class="o">-</span><span class="n">t0</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once we have the scores we can use the indices to identify the correct entries in the original data.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scores</span><span class="p">)[:</span><span class="n">num_results</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">final_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scores</span><span class="p">)[:</span><span class="n">num_results</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">feed</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now have the top 5 article entries together with their scores retrieved form 10,000 entries in the cs.CL category. Let's print out the titles, links to the full texts and the summaries.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Searched </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">num_docs</span><span class="p">)</span><span class="si">}</span><span class="s2"> documents in </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">query_time</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="si">}</span><span class="s2"> seconds</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Article titles are stored in entries[i].title</span>
<span class="c1"># Article abstracts are stored in entries[i].summary</span>
<span class="c1"># Article links are stored in entries[i].link</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">final_scores</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">title</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">link</span><span class="si">}</span><span class="se">\n\n</span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">summary</span><span class="si">}</span><span class="se">\n</span><span class="s2">[Score: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="si">}</span><span class="s2">]</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Searched 10000 documents in 0.434 seconds

Stochastic Answer Networks for Natural Language Inference
http://arxiv.org/abs/1804.07888v2

We propose a stochastic answer network (SAN) to explore multi-step inference
strategies in Natural Language Inference. Rather than directly predicting the
results given the inputs, the model maintains a state and iteratively refines
its predictions. Our experiments show that SAN achieves the state-of-the-art
results on three benchmarks: Stanford Natural Language Inference (SNLI)
dataset, MultiGenre Natural Language Inference (MultiNLI) dataset and Quora
Question Pairs dataset.
[Score: 8.8786]


Attention Boosted Sequential Inference Model
http://arxiv.org/abs/1812.01840v2

Attention mechanism has been proven effective on natural language processing.
This paper proposes an attention boosted natural language inference model named
aESIM by adding word attention and adaptive direction-oriented attention
mechanisms to the traditional Bi-LSTM layer of natural language inference
models, e.g. ESIM. This makes the inference model aESIM has the ability to
effectively learn the representation of words and model the local subsentential
inference between pairs of premise and hypothesis. The empirical studies on the
SNLI, MultiNLI and Quora benchmarks manifest that aESIM is superior to the
original ESIM model.
[Score: 8.591]


A Neural Architecture Mimicking Humans End-to-End for Natural Language
  Inference
http://arxiv.org/abs/1611.04741v2

In this work we use the recent advances in representation learning to propose
a neural architecture for the problem of natural language inference. Our
approach is aligned to mimic how a human does the natural language inference
process given two statements. The model uses variants of Long Short Term Memory
(LSTM), attention mechanism and composable neural networks, to carry out the
task. Each part of our model can be mapped to a clear functionality humans do
for carrying out the overall task of natural language inference. The model is
end-to-end differentiable enabling training by stochastic gradient descent. On
Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves
better accuracy numbers than all published models in literature.
[Score: 8.5675]


Neural Natural Language Inference Models Enhanced with External
  Knowledge
http://arxiv.org/abs/1711.04289v3

Modeling natural language inference is a very challenging task. With the
availability of large annotated data, it has recently become feasible to train
complex models such as neural-network-based inference models, which have shown
to achieve the state-of-the-art performance. Although there exist relatively
large annotated data, can machines learn all knowledge needed to perform
natural language inference (NLI) from these data? If not, how can
neural-network-based NLI models benefit from external knowledge and how to
build NLI models to leverage it? In this paper, we enrich the state-of-the-art
neural natural language inference models with external knowledge. We
demonstrate that the proposed models improve neural NLI models to achieve the
state-of-the-art performance on the SNLI and MultiNLI datasets.
[Score: 8.2145]


Baselines and test data for cross-lingual inference
http://arxiv.org/abs/1704.05347v2

The recent years have seen a revival of interest in textual entailment,
sparked by i) the emergence of powerful deep neural network learners for
natural language processing and ii) the timely development of large-scale
evaluation datasets such as SNLI. Recast as natural language inference, the
problem now amounts to detecting the relation between pairs of statements: they
either contradict or entail one another, or they are mutually neutral. Current
research in natural language inference is effectively exclusive to English. In
this paper, we propose to advance the research in SNLI-style natural language
inference toward multilingual evaluation. To that end, we provide test data for
four major languages: Arabic, French, Spanish, and Russian. We experiment with
a set of baselines. Our systems are based on cross-lingual word embeddings and
machine translation. While our best system scores an average accuracy of just
over 75%, we focus largely on enabling further research in multilingual
inference.
[Score: 8.1544]


</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see the BM25 algorithm works pretty well and the <code>rank-bm25</code> Python library is able to rank 10,000 article abstracts in just 0.434 seconds. In this demo the most time consuming part was retrieving and parsing the 10,000 entries using <code>feedparser</code>. In real life applications you would index the data to allow faster retrieval.</p>
<p>Hope you enjoyed this demo. Feel free to contact me if you have any questions.</p>
<ul>
<li>Twitter: <a href="https://twitter.com/aarnetalman">@AarneTalman</a></li>
<li>Website: <a href="https://talman.io">talman.io</a></li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="aarnetalman/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/information_retrieval/python/demo/2020/12/30/simple-search-engine-with-bm25.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Natural Language Processing and Machine Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/aarnetalman" title="aarnetalman"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/aarnetalman" title="aarnetalman"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
